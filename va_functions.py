import numpy as np
import numpy.linalg as linalg
import pandas as pd
import scipy.sparse as sps
from functools import reduce
import sys
from config_tva import *
sys.path += [hdfe_dir]
from hdfe import Groupby


def get_ll(sigma_mu_squared, sigma_theta_squared, sigma_epsilon_squared,
           n_students_per_class, n_classes, n_students, y_tilde, x_tilde,
           x_bar, y_bar, beta, beta_plus_lambda, alpha, teacher_grouped,
           return_means=False, h=None, h_sum=None, y_bar_tilde=None, x_bar_tilde=None,
           y_bar_bar=None, x_bar_bar=None):
    assert sigma_mu_squared > 0
    assert np.isscalar(sigma_mu_squared)
    assert np.isscalar(sigma_theta_squared)
    assert np.isscalar(sigma_epsilon_squared)
    assert np.isscalar(alpha)
    assert np.isscalar(n_students)
    assert n_students_per_class.shape[0] == n_classes
    assert y_tilde.shape[0] == n_students


    if h is None:
        h = 1 / (sigma_theta_squared + sigma_epsilon_squared / n_students_per_class)
    if y_bar_tilde is None or x_bar_tilde is None or h_sum is None:
        h_sum_long = teacher_grouped.apply(np.sum, h)[:, 0]
        precision_weights = h / h_sum_long
        y_bar_bar_long = teacher_grouped.apply(np.sum, 
                                              precision_weights * y_bar)[:, 0]
        
        x_bar_bar_long = teacher_grouped.apply(lambda x: np.sum(x, 0), 
                                               precision_weights[:, None] * x_bar,
                                               width=x_bar.shape[1])

        y_bar_tilde = y_bar - y_bar_bar_long
        x_bar_tilde = x_bar - x_bar_bar_long

        h_sum = h_sum_long[teacher_grouped.first_occurrences]
    if y_bar_bar is None or x_bar_bar is None:
        y_bar_bar = y_bar_bar_long[teacher_grouped.first_occurrences]
        x_bar_bar = x_bar_bar_long[teacher_grouped.first_occurrences]


    ll = (n_classes - n_students) * np.log(sigma_epsilon_squared)\
         + np.sum(np.log(h)) - np.sum(np.log(h_sum))\
         - np.sum(np.log(sigma_mu_squared + 1 / h_sum))\
         - np.sum((y_tilde[:, 0] - x_tilde.dot(beta))**2) / sigma_epsilon_squared\
         - h.dot((y_bar_tilde - x_bar_tilde.dot(beta))**2)\
         - np.dot((y_bar_bar - x_bar_bar.dot(beta_plus_lambda) - alpha)**2,
                  1 / (sigma_mu_squared + 1 / h_sum))

    assert np.isfinite(ll)
    if return_means:
        return -1 * ll / 2, h, h_sum, y_tilde, x_tilde, y_bar_tilde, x_bar_tilde, y_bar_bar, x_bar_bar
    else:
        return -1 * ll / 2


def get_ll_vec_func(n_students_per_class, n_classes, n_students, y_tilde,
                    x_tilde, x_bar, y_bar, teacher_grouped):
    k = x_bar.shape[1] 
    def f(vec, return_means=False, h=None, h_sum=None, x_bar_tilde=None,
            y_bar_tilde=None, y_bar_bar=None, x_bar_bar=None):
        assert vec[0] > 0
        return get_ll(vec[0], vec[1], vec[2], n_students_per_class, n_classes,
                      n_students, y_tilde, x_tilde, x_bar, y_bar, 
                      vec[3:3 + k], vec[3+k: 3 + 2 * k], vec[-1], teacher_grouped,
                      return_means, h, h_sum, y_bar_tilde=y_bar_tilde,
                      x_bar_tilde=x_bar_tilde, y_bar_bar=y_bar_bar, x_bar_bar=x_bar_bar)
    return f


# TODO: speed this up a lot by pre-computing y_bar_bar, etc.
def get_grad_variances(sigma_mu_squared, sigma_theta_squared, 
                       sigma_epsilon_squared, beta, beta_plus_lambda, alpha, 
                       epsilon, ll_vec_func):
    """
    ll_vec_func should have been generated by get_ll_vec_func
    """
    # print('inside gradient')
    # print(sigma_mu_squared, sigma_theta_squared, sigma_epsilon_squared)
    for elt in [sigma_mu_squared, sigma_theta_squared,
                sigma_epsilon_squared, alpha, epsilon]:
        assert np.isscalar(elt)
    vec = np.concatenate(([sigma_mu_squared, sigma_theta_squared, sigma_epsilon_squared],
                          beta, beta_plus_lambda, [alpha]))

    eye = np.eye(len(vec))

    gradient = [(ll_vec_func(vec + epsilon * eye[:, i]) - ll_vec_func(vec - epsilon * eye[:, i]))/(2 * epsilon)
                for i in range(3)]
    gradient = np.array(gradient)
    assert np.all(np.isfinite(gradient))
    return gradient


def get_grad_all(sigma_mu_squared, sigma_theta_squared, 
                 sigma_epsilon_squared, beta, beta_plus_lambda, alpha, 
                 epsilon, ll_vec_func, start=0, h=None, h_sum=None,
                  x_bar_tilde=None, y_bar_tilde=None, y_bar_bar=None, x_bar_bar=None,
                  x_tilde=None, y_tilde=None):
    """
    ll_vec_func should have been generated by get_ll_vec_func
    """
    # print('inside gradient')
    # print(sigma_mu_squared, sigma_theta_squared, sigma_epsilon_squared)
    for elt in [sigma_mu_squared, sigma_theta_squared,
                sigma_epsilon_squared, alpha, epsilon]:
        assert np.isscalar(elt)
    vec = np.concatenate(([sigma_mu_squared, sigma_theta_squared, sigma_epsilon_squared],
                          beta, beta_plus_lambda, [alpha]))

    eye = np.eye(len(vec))

    # Gradient for variances
    # gradient_num = [(ll_vec_func(vec + epsilon * eye[:, i]) - ll_vec_func(vec - epsilon * eye[:, i])) / (2 * epsilon)
    #             for i in range(len(vec))]
    # gradient_num = np.array(gradient_num)

    gradient = [None, None, None]
    for i in range(3):
        if start <= i:
            gradient[i] = (ll_vec_func(vec + epsilon * eye[:, i]) - ll_vec_func(vec - epsilon * eye[:, i]))/(2 * epsilon)

    if h is None:
        _, h, h_sum, y_tilde, x_tilde, y_bar_tilde, x_bar_tilde, y_bar_bar, x_bar_bar = ll_vec_func(vec, True)

    grad_beta = -x_tilde.T.dot(y_tilde[:, 0] - x_tilde.dot(beta)) / sigma_epsilon_squared\
                - (h[:, None] * x_bar_tilde).T.dot(y_bar_tilde - x_bar_tilde.dot(beta))

    err = y_bar_bar - x_bar_bar.dot(beta_plus_lambda) - alpha
    w = 1 / (sigma_mu_squared + 1 / h_sum[:, None])
    grad_lambda_plus_beta = -(w * x_bar_bar).T.dot(err)
    grad_alpha = -err.dot(w)

    gradient = np.concatenate((gradient, grad_beta, grad_lambda_plus_beta, grad_alpha))
    return gradient


def get_grad_vec(vec, epsilon, ll_vec_func, start=0, h=None, h_sum=None,
                  x_bar_tilde=None, y_bar_tilde=None, y_bar_bar=None, x_bar_bar=None, x_tilde=None, y_tilde=None):

    sigma_mu_squared, sigma_theta_squared, sigma_epsilon_squared = vec[:3]
    len_beta = int((len(vec) - 4) / 2)
    beta = vec[3:3 + len_beta]
    beta_plus_lambda = vec[3 + len_beta: 3 + 2 * len_beta]
    alpha = vec[-1]
    return get_grad_all(sigma_mu_squared, sigma_theta_squared, 
                 sigma_epsilon_squared, beta, beta_plus_lambda, alpha, 
                 epsilon, ll_vec_func, start, h, h_sum, x_bar_tilde=x_bar_tilde,
                 y_bar_tilde=y_bar_tilde, y_bar_bar=y_bar_bar, x_bar_bar=x_bar_bar, x_tilde=x_tilde, y_tilde=y_tilde)
 

def get_hess_all(sigma_mu_squared, sigma_theta_squared, 
                 sigma_epsilon_squared, 
                 beta, beta_plus_lambda, alpha,
                 epsilon, ll_vec_func):
    for elt in [sigma_mu_squared, sigma_theta_squared,
                sigma_epsilon_squared, alpha, epsilon]:
        assert np.isscalar(elt)

    vec = np.concatenate(([sigma_mu_squared, sigma_theta_squared, sigma_epsilon_squared], beta, beta_plus_lambda, [alpha]))


    def get_hess(i, j):
        eye = np.eye(len(vec))
        if i == j:
            d = [eye[i, :], 0, -1 * eye[i, :]]
            values = [ll_vec_func(vec + epsilon * elt) for elt in d]
            return (values[0] - 2 * values[1] + values[2]) / epsilon**2
        else:
            i_, j_ = eye[i, :], eye[j, :]
            d = [i_ + j_, j_ - i_, i_ - j_, -1 * i_ - j_]
            values = [ll_vec_func(vec + epsilon * elt) for elt in d]
            return (values[0] - values[1] - values[2] + values[3]) / (4 * epsilon**2)

    n_dim = 4 + 2 * len(beta)
    print('Need to try ', n_dim * (n_dim - 1) / 2, 'log-likelihood values')
    hessian = np.zeros((n_dim, n_dim))
    for i in range(n_dim):
        for j in range(i, n_dim):
            val = get_hess(i, j)
            hessian[i, j] = val
            hessian[j, i] = val

    return hessian


def get_hess_2(sigma_mu_squared, sigma_theta_squared, 
                 sigma_epsilon_squared, 
                 beta, beta_plus_lambda, alpha,
                 epsilon, ll_vec_func):
    for elt in [sigma_mu_squared, sigma_theta_squared,
                sigma_epsilon_squared, alpha, epsilon]:
        assert np.isscalar(elt)

    vec = np.concatenate(([sigma_mu_squared, sigma_theta_squared, sigma_epsilon_squared], 
                           beta, beta_plus_lambda, [alpha]))
    eye = np.eye(len(vec))
    hessian = np.zeros((len(vec), len(vec)))
    
    ans = ll_vec_func(vec, return_means=True)
    _, h, h_sum, y_tilde, x_tilde, y_bar_tilde, x_bar_tilde, y_bar_bar, x_bar_bar = ans

    for i in range(len(vec)):
        if i < 3:
            upper = get_grad_vec(vec + eye[:, i] * epsilon, epsilon, ll_vec_func, start=i)
            lower = get_grad_vec(vec - eye[:, i] * epsilon, epsilon, ll_vec_func, start=i)
        else:
            upper = get_grad_vec(vec + eye[:, i] * epsilon, epsilon, ll_vec_func, 
                                 start=i, h=h, h_sum=h_sum,  y_bar_tilde=y_bar_tilde,
                                 x_bar_tilde=x_bar_tilde, y_bar_bar=y_bar_bar, x_bar_bar=x_bar_bar,
                                 x_tilde=x_tilde, y_tilde=y_tilde)
            lower = get_grad_vec(vec - eye[:, i] * epsilon, epsilon, ll_vec_func, start=i,
                                 h=h, h_sum=h_sum, y_bar_tilde=y_bar_tilde,
                                 x_bar_tilde=x_bar_tilde, y_bar_bar=y_bar_bar,
                                 x_bar_bar=x_bar_bar, x_tilde=x_tilde, y_tilde=y_tilde)

        hess = (upper[i:] - lower[i:]) / (2 * epsilon)
        hessian[i, i:] = hess
        hessian[i:, i] = hess

    return hessian


toString = lambda *x: '\n'.join((str(elt) for elt in x))
# For printing stuff
# sample use
# text = Test('teaher number', teacher_number)
# text.append('observation number', observation_number, other_number)
# print(text)
class Text(object): 
    def __init__(self, *string):
        self.text = toString(*string)
    def append(self, *string):
        self.text = self.text + '\n' + toString(*string)
    def __str__(self):
        return '\n\n\n' + self.text


def make_reg_table(reg_obj, var_names, categorical_controls):
    def format(param, t_stat, se):
        if abs(t_stat) > 3.291:
            stars = '***'
        elif abs(t_stat) > 2.576:
            stars = '**'
        elif abs(t_stat) > 1.96:
            stars = '*'
        else: stars = '*'
        return (str(int(round(param * 1000)) / 1000) + stars
              , '(' + str(int(round(se * 1000)) / 1000) + ')')


    coef_col = reduce(lambda x, y: x + y
                    , (format(p, t, se) 
                       for p, t, se 
                       in zip(reg_obj.params, reg_obj.tvalues, reg_obj.bse)))
    tuples = [(name, type) for name in var_names for type in ('beta', 'se')]
    coef_col = pd.Series(coef_col, index=pd.MultiIndex.from_tuples(tuples))

    # Just keep the ones that are not categorical
    keeps = [all([cat not in t[0] for cat in categorical_controls]) 
                                  for t in tuples]
    coef_col = coef_col[keeps]
    for cat in categorical_controls:
        coef_col[(cat, 'F')] = 'X'
        #vars = [cat in v for v in var_names]
        #B = np.zeros((sum(vars), len(var_names)))
        #for i, idx in enumerate(np.where(np.array(vars))[0]):
        #    B[i, idx] = 1
        #f_results = reg_obj.f_test(B).__dict__
        #coef_col[(cat, 'F')] = f_results['fvalue'][0][0]
        #coef_col[(cat, 'p')] = '(' + str(int(round(f_results['pvalue'] * 1000)) / 1000) + ')'

    coef_col[('N', '')] = reg_obj.df_resid + len(var_names) + 1
    coef_col[('R-squared', '')] = reg_obj.rsquared
    return coef_col


# List of regression objects; list of lists of controls
def make_table(regs, controls, categorical_controls):
    tab =  pd.concat((make_reg_table(x, var_names, categorical_controls) 
                      for x, var_names in zip(regs, controls)), axis=1)
    # order columns better
    constant = tab.select(lambda x: x[0] == 'constant')
    beta_parts = tab.select(lambda x: x[1] in ('beta', 'se') 
                                      and x[0] != 'constant')

    beta_not_null = beta_parts[pd.notnull(beta_parts.iloc[:, 0])]
    beta_null = beta_parts[pd.isnull(beta_parts.iloc[:, 0])]
    F_parts = tab.select(lambda x: x[1] in ('F', 'p'))
    rest = tab.select(lambda x: x[1] == '')

    tab = pd.concat((constant, beta_not_null, beta_null, F_parts, rest))
    
    for col in tab.columns:
        tab.loc[pd.isnull(tab.loc[:, col]), col] = ''

    tab = tab.reset_index()
    tab.iloc[1:-2:2, 0] = ''

    return tab.drop('level_1', axis=1)


def estimate_var_epsilon(data):
    data = data[data['var'].notnull()]
    var_epsilon_hat = np.dot(data['var'].values, data['size'].values)\
                      /np.sum(data['size'])
    assert var_epsilon_hat > 0
    return var_epsilon_hat
    

# Mean 0, variance 1
def normalize(vector):
    vector = vector - np.mean(vector)
    return vector / np.std(vector)


# function by some internet person, not me
def remove_duplicates(seq): 
    # order preserving
    seen = {}
    result = []
    for item in seq:
        marker = item
        if marker in seen:
            continue
        seen[marker] = 1
        result.append(item)
    return result


def binscatter(x, y, nbins):
    assert len(x) == len(y)
    # sort according to x
    indices = np.argsort(x)
    x = [x[i] for i in indices]
    y = [y[i] for i in indices]
    assert x == sorted(x)
    
    bins = np.zeros(nbins)
    y_means = np.zeros(nbins)
    y_medians = np.zeros(nbins)
    y_5 = np.zeros(nbins)
    y_95 = np.zeros(nbins)
    
    for i in range(0, nbins):
        start = int(len(x) * i / nbins)
        end = int(len(x) * (i+1) / nbins)
        bins[i] = np.mean(x[start:end])
        y_means[i] = np.mean(y[start:end])
        y_medians[i] = np.median(y[start:end])
        y_5[i] = np.percentile(y[start:end], 5)
        y_95[i] = np.percentile(y[start:end], 95)

    return bins, y_means, y_medians, y_5, y_95


def check_calibration(errors, precisions):         
    mean_error = np.mean(errors)
    se = (np.var(errors) / len(errors))**.5

    standardized_errors = errors**2 * precisions
    mean_standardized_error = np.mean(standardized_errors)
    standardized_error_se = (np.var(standardized_errors) / \
                             len(standardized_errors))**.5
    assert mean_error > -3 * se
    assert mean_error < 3 * se
    assert mean_standardized_error > 1 - 2 * standardized_error_se 
    assert mean_standardized_error < 1 + 2 * standardized_error_se 


# df should actually be an array
def get_unshrunk_va(array, var_theta_hat, var_epsilon_hat, jackknife):
    if jackknife:
        unshrunk = np.array(np.sum(array[:, 1]) - array[:, 1]) / (len(array)-1)
    else:
        unshrunk = np.mean(array[:, 1])

    return unshrunk


def get_va(df, var_theta_hat, var_epsilon_hat, var_mu_hat, jackknife):
    assert(var_mu_hat > 0)
    array = df.values
    precisions = np.array([1 / (var_theta_hat + var_epsilon_hat / class_size) 
                          for class_size in array[:, 0]])
    numerators = precisions * array[:, 1]
    precision_sum = np.sum(precisions)
    num_sum = np.sum(numerators)
    if jackknife:
        denominators = np.array([precision_sum - p for p in precisions]) \
                 + 1 / var_mu_hat
        return_val =[[(num_sum - n) / d, 1 / d]
                         for n, d in zip(numerators, denominators)]
    else:
        denominator = precision_sum + 1 / var_mu_hat
        return_val = [[num_sum / denominator, 1 / denominator]]

    return pd.DataFrame(data=np.array(return_val))
